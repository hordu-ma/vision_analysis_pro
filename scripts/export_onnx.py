"""YOLO æ¨¡å‹ ONNX å¯¼å‡ºè„šæœ¬

å°† PyTorch æ¨¡å‹ (.pt) å¯¼å‡ºä¸º ONNX æ ¼å¼ (.onnx)ï¼Œç”¨äºè·¨å¹³å°éƒ¨ç½²å’Œæ¨ç†åŠ é€Ÿã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/export_onnx.py
    python scripts/export_onnx.py --model runs/train/exp/weights/best.pt --output models/best.onnx
    python scripts/export_onnx.py --model yolov8n.pt --simplify --half

å¯¼å‡ºåå¯ä½¿ç”¨ ONNX Runtime è¿›è¡Œæ¨ç†ã€‚
"""

import argparse
from pathlib import Path

from ultralytics import YOLO


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="å°† YOLO PyTorch æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å¯¼å‡ºé»˜è®¤è®­ç»ƒæ¨¡å‹
  python scripts/export_onnx.py

  # å¯¼å‡ºæŒ‡å®šæ¨¡å‹åˆ°æŒ‡å®šä½ç½®
  python scripts/export_onnx.py --model runs/train/exp/weights/best.pt --output models/best.onnx

  # å¯¼å‡ºå¹¶ç®€åŒ– ONNX æ¨¡å‹
  python scripts/export_onnx.py --simplify

  # å¯¼å‡º FP16 åŠç²¾åº¦æ¨¡å‹ï¼ˆéœ€è¦ GPUï¼‰
  python scripts/export_onnx.py --half
        """,
    )

    # è¾“å…¥è¾“å‡ºé…ç½®
    parser.add_argument(
        "--model",
        type=str,
        default="runs/train/exp/weights/best.pt",
        help="è¾“å…¥ PyTorch æ¨¡å‹è·¯å¾„ (.pt)",
    )

    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="è¾“å‡º ONNX æ¨¡å‹è·¯å¾„ï¼ˆé»˜è®¤ä¸è¾“å…¥åŒç›®å½•åŒåï¼‰",
    )

    # å¯¼å‡ºå‚æ•°
    parser.add_argument(
        "--imgsz",
        type=int,
        nargs="+",
        default=[640],
        help="è¾“å…¥å›¾åƒå°ºå¯¸ï¼Œå•å€¼æˆ– [height, width]",
    )

    parser.add_argument(
        "--batch",
        type=int,
        default=1,
        help="æ‰¹æ¬¡å¤§å°ï¼ˆé™æ€å¯¼å‡ºï¼‰",
    )

    parser.add_argument(
        "--dynamic",
        action="store_true",
        help="å¯ç”¨åŠ¨æ€è¾“å…¥å°ºå¯¸ï¼ˆbatch, height, widthï¼‰",
    )

    parser.add_argument(
        "--half",
        action="store_true",
        help="å¯¼å‡º FP16 åŠç²¾åº¦æ¨¡å‹ï¼ˆéœ€è¦ GPU æ”¯æŒï¼‰",
    )

    parser.add_argument(
        "--simplify",
        action="store_true",
        help="ä½¿ç”¨ onnxslim ç®€åŒ–æ¨¡å‹ï¼ˆéœ€å®‰è£… onnxslimï¼‰",
    )

    parser.add_argument(
        "--opset",
        type=int,
        default=17,
        help="ONNX opset ç‰ˆæœ¬ï¼ˆé»˜è®¤ 17ï¼‰",
    )

    # è®¾å¤‡é…ç½®
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        help="å¯¼å‡ºè®¾å¤‡ (cpu, 0, cuda:0 ç­‰)",
    )

    # éªŒè¯é…ç½®
    parser.add_argument(
        "--no-verify",
        action="store_true",
        help="è·³è¿‡å¯¼å‡ºåçš„éªŒè¯æ­¥éª¤",
    )

    return parser.parse_args()


def export_onnx(
    model_path: str,
    output_path: str | None = None,
    imgsz: list[int] | None = None,
    batch: int = 1,
    dynamic: bool = False,
    half: bool = False,
    simplify: bool = False,
    opset: int = 17,
    device: str = "cpu",
) -> Path:
    """å¯¼å‡º ONNX æ¨¡å‹

    Args:
        model_path: è¾“å…¥ PyTorch æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡º ONNX è·¯å¾„ï¼ˆNone åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        imgsz: è¾“å…¥å›¾åƒå°ºå¯¸
        batch: æ‰¹æ¬¡å¤§å°
        dynamic: æ˜¯å¦å¯ç”¨åŠ¨æ€å°ºå¯¸
        half: æ˜¯å¦ä½¿ç”¨ FP16
        simplify: æ˜¯å¦ç®€åŒ–æ¨¡å‹
        opset: ONNX opset ç‰ˆæœ¬
        device: å¯¼å‡ºè®¾å¤‡

    Returns:
        å¯¼å‡ºçš„ ONNX æ–‡ä»¶è·¯å¾„
    """
    if imgsz is None:
        imgsz = [640]

    model_path = Path(model_path)
    if not model_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    model = YOLO(str(model_path))

    print("ğŸ”„ å¼€å§‹å¯¼å‡º ONNX...")

    # è°ƒç”¨ Ultralytics çš„ export æ–¹æ³•
    export_path = model.export(
        format="onnx",
        imgsz=imgsz,
        batch=batch,
        dynamic=dynamic,
        half=half,
        simplify=simplify,
        opset=opset,
        device=device,
    )

    export_path = Path(export_path)

    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œç§»åŠ¨æ–‡ä»¶
    if output_path is not None:
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        if export_path != output_path:
            import shutil

            shutil.move(str(export_path), str(output_path))
            export_path = output_path
            print(f"ğŸ“ ç§»åŠ¨åˆ°: {output_path}")

    return export_path


def verify_onnx(onnx_path: Path) -> bool:
    """éªŒè¯å¯¼å‡ºçš„ ONNX æ¨¡å‹

    Args:
        onnx_path: ONNX æ¨¡å‹è·¯å¾„

    Returns:
        éªŒè¯æ˜¯å¦é€šè¿‡
    """
    print("\nğŸ” éªŒè¯ ONNX æ¨¡å‹...")

    try:
        import onnx

        # åŠ è½½å¹¶æ£€æŸ¥æ¨¡å‹
        model = onnx.load(str(onnx_path))
        onnx.checker.check_model(model)

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print("  âœ… ONNX æ¨¡å‹æ ¼å¼æ­£ç¡®")

        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        graph = model.graph

        print("\n  ğŸ“¥ è¾“å…¥:")
        for inp in graph.input:
            shape = [
                d.dim_value if d.dim_value > 0 else d.dim_param
                for d in inp.type.tensor_type.shape.dim
            ]
            print(f"     - {inp.name}: {shape}")

        print("\n  ğŸ“¤ è¾“å‡º:")
        for out in graph.output:
            shape = [
                d.dim_value if d.dim_value > 0 else d.dim_param
                for d in out.type.tensor_type.shape.dim
            ]
            print(f"     - {out.name}: {shape}")

        # æ‰“å°æ–‡ä»¶å¤§å°
        file_size = onnx_path.stat().st_size / (1024 * 1024)
        print(f"\n  ğŸ“Š æ–‡ä»¶å¤§å°: {file_size:.2f} MB")

        return True

    except ImportError:
        print("  âš ï¸  æœªå®‰è£… onnx åº“ï¼Œè·³è¿‡è¯¦ç»†éªŒè¯")
        print("     å®‰è£…: uv add onnx")
        return True

    except Exception as e:
        print(f"  âŒ éªŒè¯å¤±è´¥: {e}")
        return False


def main() -> None:
    """ä¸»å‡½æ•°"""
    args = parse_args()

    print("=" * 60)
    print("ğŸš€ YOLO æ¨¡å‹ ONNX å¯¼å‡ºå·¥å…·")
    print("=" * 60)

    # æ‰“å°é…ç½®
    print("\nğŸ“‹ å¯¼å‡ºé…ç½®:")
    print(f"  è¾“å…¥æ¨¡å‹:   {args.model}")
    print(f"  è¾“å‡ºè·¯å¾„:   {args.output or '(è‡ªåŠ¨ç”Ÿæˆ)'}")
    print(f"  å›¾åƒå°ºå¯¸:   {args.imgsz}")
    print(f"  æ‰¹æ¬¡å¤§å°:   {args.batch}")
    print(f"  åŠ¨æ€å°ºå¯¸:   {args.dynamic}")
    print(f"  åŠç²¾åº¦:     {args.half}")
    print(f"  ç®€åŒ–æ¨¡å‹:   {args.simplify}")
    print(f"  Opset:      {args.opset}")
    print(f"  å¯¼å‡ºè®¾å¤‡:   {args.device}")

    print("\n" + "-" * 60)

    try:
        # æ‰§è¡Œå¯¼å‡º
        onnx_path = export_onnx(
            model_path=args.model,
            output_path=args.output,
            imgsz=args.imgsz,
            batch=args.batch,
            dynamic=args.dynamic,
            half=args.half,
            simplify=args.simplify,
            opset=args.opset,
            device=args.device,
        )

        print(f"\nâœ… å¯¼å‡ºæˆåŠŸ: {onnx_path}")

        # éªŒè¯æ¨¡å‹
        if not args.no_verify:
            verify_onnx(onnx_path)

        print("\n" + "=" * 60)
        print("ğŸ‰ ONNX å¯¼å‡ºå®Œæˆï¼")
        print("=" * 60)

        # ä½¿ç”¨æç¤º
        print("\nğŸ“ åç»­æ­¥éª¤:")
        print("  1. ä½¿ç”¨ ONNX Runtime æ¨ç†:")
        print("     import onnxruntime as ort")
        print(f"     session = ort.InferenceSession('{onnx_path}')")
        print("\n  2. æˆ–å¤åˆ¶åˆ° models/ ç›®å½•:")
        print(f"     cp {onnx_path} models/best.onnx")

    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("   è¯·ç¡®è®¤æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œæˆ–å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬")
        raise SystemExit(1) from e

    except Exception as e:
        print(f"\nâŒ å¯¼å‡ºå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
